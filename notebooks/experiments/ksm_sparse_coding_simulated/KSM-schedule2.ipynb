{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Description\n",
    "\n",
    "Goals of the notebook:\n",
    "- Generate a high dimensional dataset using sklearn\n",
    "- Use the algorithm to learn representations on that\n",
    "- Testing KSM with $P$ being tracked\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "import hydra\n",
    "import scipy.linalg as la\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "from tqdm import tqdm, trange\n",
    "from src.plot_utils import scatter_plot_2d\n",
    "from src.utils import sparsity_measure\n",
    "from omegaconf import OmegaConf\n",
    "from torchvision.utils import make_grid\n",
    "from pprint import pprint\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from sklearn.decomposition import MiniBatchDictionaryLearning, SparseCoder, DictionaryLearning\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style('ticks')\n",
    "sns.set_context(\"paper\", font_scale=1.0, rc={\"text.usetex\": True})\n",
    "sns.set_palette(\"colorblind\")\n",
    "sns.set_style({\"font.family\": \"serif\", \"font.serif\": [\"Times New Roman\"]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_dir = Path('/n/home13/shubham/Current Projects/bioplausible_learning/code')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dataset': {'batch_size': 128,\n",
      "             'dim': 30,\n",
      "             'n_samples': 1500,\n",
      "             'name': 'highdim'},\n",
      " 'device': 'device',\n",
      " 'model': {'K': 16,\n",
      "           'Winit': 'Kmeans',\n",
      "           'lam': 0.5,\n",
      "           'omega': 0.0001,\n",
      "           'perturbation': 0.1,\n",
      "           'rho': 1.0,\n",
      "           'sparsity': 0.125},\n",
      " 'optimizer': {'Minit': 'zero',\n",
      "               'latent_iters': 15,\n",
      "               'log_interval': 5,\n",
      "               'lrs': {'Z': 0.01,\n",
      "                       'Z_decay': 0.9,\n",
      "                       'interval': 10,\n",
      "                       'params': 0.01},\n",
      "               'max_epochs': 3000,\n",
      "               'param_iters': 1,\n",
      "               'vis_interval': 20}}\n"
     ]
    }
   ],
   "source": [
    "with hydra.initialize(version_base=None, config_path=\"../../../configs\"):\n",
    "    cfg = hydra.compose(config_name='ksm_high_dim.yaml')\n",
    "    exp_params = OmegaConf.to_container(cfg, resolve=True)\n",
    "    \n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "exp_params['device'] = 'device'\n",
    "\n",
    "# figfolder = Path(f'../../results/simulated/highdim/{datetime.now().strftime(\"%m-%d\")}/{datetime.now().strftime(\"%H-%M\")}')\n",
    "# os.makedirs(figfolder, exist_ok=True)    \n",
    "pprint(exp_params)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simulated data generated from a tall dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_sparse_coded_signal\n",
    "\n",
    "def get_sim_data(n_samples, n_components, n_features, n_nonzero_coefs, max_correlation=0.3, noise=None, seed=42):\n",
    "    \n",
    "    np.random.seed(seed)\n",
    "    positions = np.random.randint(0, high=n_components, size=(n_samples, n_nonzero_coefs))\n",
    "    vals = 5 + (-1)*np.random.rand(n_samples, n_nonzero_coefs)\n",
    "    Z = np.zeros((n_samples, n_components))\n",
    "    Z[np.arange(n_samples)[:, None], positions] = vals\n",
    "    \n",
    "    # Create D with max correlation\n",
    "    # random_matrix = 2*(max_correlation) * np.random.rand(n_components, n_components) - max_correlation\n",
    "    # lower_triangle = np.tril(random_matrix, k=0)\n",
    "    # print(lower_triangle)\n",
    "    # cov = lower_triangle @ lower_triangle.T + np.eye(n_components)\n",
    "                                   \n",
    "    # cov = random_matrix + random_matrix.T\n",
    "    \n",
    "    cov = np.eye(n_components)\n",
    "    # np.fill_diagonal(cov, 1)\n",
    "    \n",
    "    D = np.random.multivariate_normal(np.zeros(n_components), cov, size=n_features).T\n",
    "    # print(np.linalg.norm(D, axis=1, keepdims=True))\n",
    "    D /= np.linalg.norm(D, axis=1, keepdims=True)\n",
    "    X = Z @ D\n",
    "    \n",
    "    \n",
    "    \n",
    "    # X, D, Z = make_sparse_coded_signal(n_samples=n_samples, n_components=n_components, n_features=n_features, n_nonzero_coefs=n_nonzero_coefs, random_state=seed)\n",
    "    \n",
    "    if noise is not None:\n",
    "        # Adding Gaussian Noise\n",
    "        X += noise * np.random.randn(*X.shape)    \n",
    "    \n",
    "    return torch.from_numpy(X).float(), torch.from_numpy(D).float(), torch.from_numpy(Z).float()\n",
    "\n",
    "\n",
    "X, D, Z = get_sim_data(n_samples=exp_params['dataset']['n_samples'],\n",
    "                       n_components=exp_params['model']['K'],\n",
    "                       n_features=exp_params['dataset']['dim'],\n",
    "                       n_nonzero_coefs=np.ceil(exp_params['model']['K']*exp_params['model']['sparsity']).astype(int),\n",
    "                       seed=42)\n",
    "\n",
    "# Create pytorch dataset and dataloader\n",
    "indices = torch.arange(X.shape[0])\n",
    "dataset = TensorDataset(X, indices)\n",
    "dataloader = DataLoader(dataset, batch_size=exp_params['dataset']['batch_size'], )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize dictionary D with colorbar\n",
    "fig, ax = plt.subplots(1, 1, figsize=(3, 5))\n",
    "im = ax.imshow(D.T, aspect='auto')\n",
    "ax.set_title(f'Dictionary D ({D.shape[1]}x{D.shape[0]})')\n",
    "ax.grid(False)\n",
    "fig.colorbar(im, ax=ax, orientation='vertical')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max correlation between columns\n",
      "Max: 0.48\n"
     ]
    }
   ],
   "source": [
    "print(\"Max correlation between columns\")\n",
    "print(f\"Max: {torch.max(torch.abs(D @ D.T - torch.eye(D.shape[0]))):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Cluster centers for the given data\n",
    "kmeans = KMeans(n_clusters=exp_params['model']['K'], random_state=42).fit(X)\n",
    "W_init = kmeans.cluster_centers_\n",
    "W_init /= np.linalg.norm(W_init, axis=1, keepdims=True)\n",
    "\n",
    "# sim_lam = 2*exp_params['model']['lam']\n",
    "# print(sim_lam)\n",
    "coder = SparseCoder(dictionary=W_init, transform_alpha=exp_params['model']['lam'], transform_algorithm='lasso_lars')\n",
    "init_codes = coder.transform(X.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading learnt values from sklearn dictionary learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = f\"learnt_normal_dict_K{exp_params['model']['K']}_lam{0.1:.2e}_n{exp_params['dataset']['dim']}.pkl\"\n",
    "folder = project_dir / 'data/simulated/'\n",
    "with open(folder / filename, 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "    synthetic_dict = data['obj']\n",
    "    Z = data['Z']\n",
    "    X = data['X']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_reconstruction_error(X, D, Z):\n",
    "    \"\"\"\n",
    "    Compute the reconstruction error of the given data\n",
    "    X: (n_samples, n_features)\n",
    "    D: (n_components, n_features)\n",
    "    Z: (n_samples, n_components)\n",
    "    \"\"\"\n",
    "    prediction = Z @ D\n",
    "    error = np.mean(np.linalg.norm(X - prediction, axis=1)**2)\n",
    "    return error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 Score: 0.9994\n",
      "Sparsity level: 0.86\n"
     ]
    }
   ],
   "source": [
    "# Print the r2 score\n",
    "Z_est = synthetic_dict.transform(X)\n",
    "r2 = r2_score(X, Z_est @ synthetic_dict.components_)\n",
    "print(f\"R2 Score: {r2:0.4f}\")\n",
    "\n",
    "# Calculate sparsity level\n",
    "# sparsity = np.mean(np.sum(Z_est != 0, axis=1)) / exp_params['model']['K']\n",
    "sparsity = sparsity_measure(torch.from_numpy(Z_est))\n",
    "print(f\"Sparsity level: {sparsity:0.2f}\")\n",
    "\n",
    "synthetic_dict_output ={\n",
    "    'D': torch.from_numpy(synthetic_dict.components_).float(),\n",
    "    'Z': torch.from_numpy(Z_est).float(),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.visualizations import visualize_latents\n",
    "from src.utils import realign_dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Align dictionaries\n",
    "cos_sim = np.abs(synthetic_dict.components_ @ D.numpy().T)\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 10))\n",
    "sns.heatmap(cos_sim, ax=ax, cmap='coolwarm', annot=True, fmt='.2f')\n",
    "ax.set_title('Cos similarity between true and estimated dictionary')\n",
    "ax.set_xlabel('True Dictionary')\n",
    "ax.set_ylabel('Estimated Dictionary')\n",
    "fig.show()\n",
    "\n",
    "positions = realign_dictionaries(cos_sim)\n",
    "print(positions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run gradient dynamics of KSM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models import KSM_objective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initializeTensors(exp_params, n_samples, Zinit=None, Pinit=None, Minit=None, seed=42, device='cpu'):\n",
    "    torch.manual_seed(seed)\n",
    "    if Zinit is None:\n",
    "        Z = torch.randn(size=(n_samples, exp_params['model']['K']), dtype=torch.float32, requires_grad=False)\n",
    "    else:\n",
    "        Z = Zinit.clone()\n",
    "        \n",
    "        \n",
    "    if Pinit is None:    \n",
    "        Z_t = Z.unsqueeze(-1)\n",
    "        V = Z_t @ Z_t.transpose(-1, -2)\n",
    "        P = V + exp_params['model']['omega'] * torch.eye(exp_params['model']['K'], dtype=torch.float32, requires_grad=False, device=device)\n",
    "    else:\n",
    "        P = Pinit.clone()\n",
    "        \n",
    "        \n",
    "    if Minit is None:        \n",
    "        M_t = torch.randn(size=P.shape, requires_grad=False, dtype=torch.float32)\n",
    "        M = M_t @ M_t.transpose(-1, -2)\n",
    "        M = M / torch.linalg.norm(M, dim=(1, 2), keepdims=True)\n",
    "    else:\n",
    "        M = Minit.clone()\n",
    "        \n",
    "        \n",
    "    # print(\"Initialized M\")\n",
    "    \n",
    "    variables = {'Z': Z, 'P': P}\n",
    "    lagrange = {'M': M}\n",
    "    \n",
    "    return variables, lagrange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create results folder\n",
    "current_path = Path.cwd()\n",
    "results_folder = current_path / 'results'/f'{datetime.now().strftime(\"%m-%d\")}/Ptrack'\n",
    "os.makedirs(results_folder, exist_ok=True)\n",
    "from copy import deepcopy\n",
    "cfg = deepcopy(exp_params)\n",
    "\n",
    "# folder = project_dir / f'results/simulated/highdim/{datetime.now().strftime(\"%m-%d\")}/{datetime.now().strftime(\"%H-%M\")}'\n",
    "os.makedirs(results_folder, exist_ok=True)\n",
    "cfg.pop('device')\n",
    "cfg = OmegaConf.create(cfg)\n",
    "with open(results_folder / 'config.yaml', 'w') as f:\n",
    "    OmegaConf.save(cfg, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_params['device'] = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device = exp_params['device']\n",
    "# M_init = torch.zeros(size=(exp_params['dataset']['n_samples'], exp_params['model']['K'], exp_params['model']['K']), dtype=torch.float32, requires_grad=False).to(device)\n",
    "W = torch.from_numpy(W_init).float()\n",
    "M_init = W @ W.T / 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.optimizers import optimizationADMM_proximal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "M_init =  M_init.repeat(exp_params['dataset']['n_samples'], 1, 1).to(device)\n",
    "Z_init = torch.from_numpy(init_codes).float().to(device)\n",
    "variables, lagrange = initializeTensors(exp_params, n_samples=exp_params['dataset']['n_samples'], Zinit=Z_init, Minit=M_init, seed=42, device=device)\n",
    "exp_params['optimizer']['max_epochs'] = 3500\n",
    "exp_params['optimizer']['latent_iters'] = 15\n",
    "exp_params['optimizer']['log_interval'] = 10\n",
    "exp_params['optimizer']['lrs']['params'] = 1e-2\n",
    "exp_params['optimizer']['param_iters'] = 1\n",
    "exp_params['model']['lam'] = 0.5\n",
    "model = KSM_objective(exp_params, W_init=torch.from_numpy(W_init).float())\n",
    "optimizer = optimizationADMM_proximal(exp_params, model, eta=0.05)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.admms import run_ADMM_Ptrack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/3500 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 3500/3500 [17:10<00:00,  3.40it/s, Total Loss: -16.3583 | Similarity Loss: -20.3148 | L1 Loss: 0.9192 | Penalty: 0.0000 | R2_W: 0.9847 | R2_est: 0.9992 | D_sim: 0.9298 | Z_sim: 0.8379 | DW_sim: 0.9297 | new_eta: 39.5179]\n"
     ]
    }
   ],
   "source": [
    "true_vals = {'D': D, 'Z': Z}\n",
    "\n",
    "loss_vals = run_ADMM_Ptrack(model, optimizer, X, variables, lagrange, dataloader, exp_params, \n",
    "                    true_vals=synthetic_dict_output, device=device, update_eta=True, normW=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's plot the losses\n",
    "fig, axs = plt.subplots(1, 4, figsize=(20, 5))\n",
    "iterations = np.arange(0, exp_params['optimizer']['max_epochs'], step=exp_params['optimizer']['log_interval'])\n",
    "ax = axs[0]\n",
    "ax.plot(iterations[1:], loss_vals['total_loss'][1:])\n",
    "ax.set_title('Total Loss')\n",
    "# ax.set_xscale('log')\n",
    "ax.set_xlabel('Iterations')\n",
    "ax.set_ylabel('Loss')\n",
    "\n",
    "ax = axs[1]\n",
    "ax.plot(iterations[1:], loss_vals['similarity_loss'][1:])\n",
    "ax.set_title('Similarity Loss')\n",
    "# ax.set_xscale('log')\n",
    "ax.set_xlabel('Iterations')\n",
    "\n",
    "ax = axs[2]\n",
    "ax.plot(iterations, loss_vals['sparsity'])\n",
    "ax.set_title('Sparsity Measure')\n",
    "# ax.set_xscale('log')\n",
    "ax.set_xlabel('Iterations')\n",
    "\n",
    "ax = axs[3]\n",
    "ax.plot(iterations, loss_vals['penalty'])\n",
    "ax.set_title('Penalty')\n",
    "# ax.set_xscale('log')\n",
    "ax.set_xlabel('Iterations')\n",
    "\n",
    "# fig.savefig(results_folder / 'loss_curves.pdf', format='pdf', bbox_inches='tight')\n",
    "\n",
    "\n",
    "# ax.plot(loss_vals['loss_Z'])\n",
    "\n",
    "\n",
    "# Plot the r2 score and similarity measures\n",
    "fig, axs = plt.subplots(1, 3, figsize=(15, 5))\n",
    "ax = axs[0]\n",
    "ax.plot(iterations[1:], loss_vals['r2_W'][1:], label=r'$R^2 (W)$')\n",
    "ax.plot(iterations[1:], loss_vals['r2_est'][1:], label=r'$R^2$ (Estimated)')\n",
    "# ax.set_xscale('log')\n",
    "ax.set_title('R2 Score')\n",
    "ax.set_xlabel('Iterations')\n",
    "ax.set_ylabel('R2 Score')\n",
    "ax.legend()\n",
    "\n",
    "ax = axs[1]\n",
    "ax.plot(iterations, loss_vals['dw_sim'], label='cos sim (W)')\n",
    "ax.plot(iterations, loss_vals['dest_sim'], label='cos sim (est)')\n",
    "# ax.set_xscale('log')\n",
    "ax.set_title('Cos Similarity for estimated dictionary')\n",
    "ax.set_xlabel('Iterations')\n",
    "ax.set_ylabel('Value')\n",
    "ax.legend()\n",
    "\n",
    "ax = axs[2]\n",
    "ax.plot(iterations, loss_vals['latent_sim'])\n",
    "# ax.set_xscale('log')\n",
    "ax.set_title('Cos Similarity for latent codes')\n",
    "ax.set_xlabel('Iterations')\n",
    "ax.set_ylabel('Value')\n",
    "\n",
    "fig.savefig(results_folder / 'r2_n_cossim.pdf', format='pdf', bbox_inches='tight')\n",
    "\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot dictionaries side by side\n",
    "fig, axs = plt.subplots(1, 3, figsize=(15, 5))\n",
    "D_W = model.W.cpu().numpy()\n",
    "H_est = torch.mean(variables['P'], dim=0)\n",
    "Z_est = variables['Z']\n",
    "D_est = torch.linalg.solve(H_est, Z_est.T).cpu() @ X / X.shape[0]\n",
    "D_est = D_est.numpy()\n",
    "\n",
    "ax = axs[0]\n",
    "im = ax.imshow(true_vals['D'].T, aspect='auto')\n",
    "ax.set_title('True Dictionary')\n",
    "ax.grid(False)\n",
    "fig.colorbar(im, ax=ax, orientation='vertical')\n",
    "\n",
    "ax = axs[1]\n",
    "im = ax.imshow(D_W.T, aspect='auto')\n",
    "ax.grid(False)\n",
    "ax.set_title(r'$W^T$')\n",
    "fig.colorbar(im, ax=ax, orientation='vertical')\n",
    "\n",
    "ax = axs[2]\n",
    "im = ax.imshow(D_est.T, aspect='auto')\n",
    "ax.grid(False)\n",
    "ax.set_title('Estimated Dictionary')\n",
    "fig.colorbar(im, ax=ax, orientation='vertical')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Align dictionaries\n",
    "D_W = model.W.cpu().numpy()\n",
    "D_W_norm = D_W / np.linalg.norm(D_W, axis=1, keepdims=True)\n",
    "H_est_ksm = torch.mean(variables['P'], dim=0)\n",
    "Z_est_ksm = variables['Z']\n",
    "D_est_ksm = torch.linalg.solve(H_est_ksm, Z_est_ksm.T).cpu() @ X / X.shape[0]\n",
    "D_est_ksm = D_est_ksm.numpy()\n",
    "D_est_ksm_norm = D_est_ksm / np.linalg.norm(D_est_ksm, axis=1, keepdims=True)\n",
    "cos_sim_ksm = np.abs(D_est_ksm_norm @ D.numpy().T)\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 10))\n",
    "sns.heatmap(cos_sim_ksm, ax=ax, cmap='coolwarm', annot=True, fmt='.2f')\n",
    "ax.set_title('Cos similarity between true and estimated dictionary')\n",
    "ax.set_xlabel('True Dictionary')\n",
    "ax.set_ylabel('Estimated Dictionary')\n",
    "fig.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils import realign_dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 9  1  0 13  2 12 14  3 15  4  8  5  6 10 11  7]\n"
     ]
    }
   ],
   "source": [
    "positions_ksm = realign_dictionaries(cos_sim_ksm)\n",
    "print(positions_ksm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the dictionary\n",
    "D_est_realigned_ksm = D_est_ksm_norm[positions_ksm]\n",
    "W_init_realigned_ksm = W_init[positions_ksm]\n",
    "n_rows = 4\n",
    "n_cols = D.shape[0] // n_rows\n",
    "fig, ax = plt.subplots(n_rows, D.shape[0] // n_cols, figsize=(4*n_cols, 4*n_rows))\n",
    "ax = ax.ravel()\n",
    "for i in range(D.shape[0]):\n",
    "    ax[i].plot(np.abs(D_est_realigned_ksm[i]), label='Estimated', c='g', linestyle='-', marker='x')\n",
    "    ax[i].plot(np.abs(D[i]), label='True', c='r', linestyle='--', marker='o', alpha=0.9)\n",
    "    ax[i].plot(np.abs(W_init_realigned_ksm[i]), label='Initial', c='b', linestyle='-.', marker='s', alpha=0.3)\n",
    "    # ax[i].plot(D[i], label='True')\n",
    "    # ax[i].plot(W_init_realigned[i], label='KMeans')\n",
    "    ax[i].set_title(f'Component {i}')\n",
    "ax[0].legend()\n",
    "fig.suptitle('Dictionary Components from KSM')\n",
    "fig.tight_layout()\n",
    "fig.savefig(results_folder / 'dictionary_components_ksm.pdf', format='pdf', bbox_inches='tight')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the dictionary\n",
    "D_est_realigned_ksm = D_est_ksm[positions_ksm]\n",
    "W_init_realigned_ksm = W_init[positions_ksm]\n",
    "n_rows = 4\n",
    "n_cols = D.shape[0] // n_rows\n",
    "fig, ax = plt.subplots(n_rows, n_cols, figsize=(4*n_cols, 4*n_rows))\n",
    "ax = ax.ravel()\n",
    "for i in range(D.shape[0]):\n",
    "    ax[i].plot(np.abs(D_est_realigned_ksm[i]), label='Estimated', c='g', linestyle='-', marker='x')\n",
    "    ax[i].plot(np.abs(D[i]), label='True', c='r', linestyle='--', marker='o', alpha=0.9)\n",
    "    ax[i].plot(np.abs(W_init_realigned_ksm[i]), label='Initial', c='b', linestyle='-.', marker='s', alpha=0.3)\n",
    "    # ax[i].plot(D[i], label='True')\n",
    "    # ax[i].plot(W_init_realigned[i], label='KMeans')\n",
    "    ax[i].set_title(f'Component {i}')\n",
    "ax[0].legend()\n",
    "fig.suptitle('Dictionary Components from KSM (not normalized)')\n",
    "fig.tight_layout()\n",
    "fig.savefig(results_folder / 'dictionary_components_ksm_not_normalized.pdf', format='pdf', bbox_inches='tight')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "filename = f\"ksm_Ptrack_K{exp_params['model']['K']}_lam{exp_params['model']['lam']:.2e}_n{exp_params['dataset']['dim']}_epochs{exp_params['optimizer']['max_epochs']}.pkl\"\n",
    "with open(results_folder / filename, 'wb') as f:\n",
    "    save_data = {'model': model, \n",
    "                 'variables': variables, \n",
    "                 'lagrange': lagrange, \n",
    "                 'loss_vals': loss_vals, \n",
    "                 'true_vals': true_vals, \n",
    "                 'X': X}\n",
    "    pickle.dump(save_data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.visualizations import visualize_latents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize latents\n",
    "true_Z = Z.numpy()\n",
    "Z_est_ksm = variables['Z'].cpu().detach().numpy()\n",
    "Z_init = Z_init.cpu().numpy()\n",
    "fig, ax = plt.subplots(5, 1, figsize=(4, 10))\n",
    "ax = visualize_latents(true_Z, ax=ax, seed=42, linefmt='r--', markerfmt='ro', basefmt='r--', label='True')\n",
    "ax = visualize_latents(Z_init[:, positions_ksm], ax=ax, seed=42, linefmt='b-.', markerfmt='bs', basefmt='b-.', label='Initial')\n",
    "ax = visualize_latents(Z_est_ksm[:, positions_ksm], ax=ax, seed=42, linefmt='g-', markerfmt='gx', basefmt='g-', label='Estimated')\n",
    "\n",
    "ax[0].legend()\n",
    "fig.tight_layout()\n",
    "fig.savefig(results_folder / 'estimated_latents.pdf', format='pdf', bbox_inches='tight')\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bioplausible-learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
