{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "import hydra\n",
    "import scipy.linalg as la\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "from tqdm import tqdm, trange\n",
    "from src.plot_utils import scatter_plot_2d\n",
    "from src.utils import sparsity_measure\n",
    "from omegaconf import OmegaConf\n",
    "from torchvision.utils import make_grid\n",
    "from pprint import pprint\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from sklearn.decomposition import MiniBatchDictionaryLearning, SparseCoder, DictionaryLearning\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style('ticks')\n",
    "sns.set_context(\"paper\", font_scale=1.0, rc={\"text.usetex\": True})\n",
    "sns.set_palette(\"colorblind\")\n",
    "sns.set_style({\"font.family\": \"serif\", \"font.serif\": [\"Times New Roman\"]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_dir = Path('/n/home13/shubham/Current Projects/bioplausible_learning/code')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dataset': {'batch_size': 128, 'dim': 30, 'n_samples': 2500, 'name': 'moons'},\n",
      " 'device': 'device',\n",
      " 'model': {'K': 12,\n",
      "           'Winit': 'Kmeans',\n",
      "           'omega': 0.1,\n",
      "           'perturbation': 0.1,\n",
      "           'rho': 1.0,\n",
      "           'sparsity': 0.125},\n",
      " 'optimizer': {'Minit': 'zero',\n",
      "               'latent_iters': 20,\n",
      "               'log_interval': 5,\n",
      "               'lrs': {'Z': 0.01,\n",
      "                       'Z_decay': 0.9,\n",
      "                       'interval': 20,\n",
      "                       'params': 0.01},\n",
      "               'max_epochs': 3000,\n",
      "               'param_iters': 1,\n",
      "               'vis_interval': 20}}\n"
     ]
    }
   ],
   "source": [
    "with hydra.initialize(version_base=None, config_path=\"../../../configs\"):\n",
    "    cfg = hydra.compose(config_name='ksm_moons.yaml')\n",
    "    exp_params = OmegaConf.to_container(cfg, resolve=True)\n",
    "    \n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "exp_params['device'] = 'device'\n",
    "\n",
    "# figfolder = Path(f'../../results/simulated/highdim/{datetime.now().strftime(\"%m-%d\")}/{datetime.now().strftime(\"%H-%M\")}')\n",
    "# os.makedirs(figfolder, exist_ok=True)    \n",
    "pprint(exp_params)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simulated data generated from a tall dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "X, y = make_moons(n_samples=exp_params['dataset']['n_samples'], noise=0.01)\n",
    "fig, ax = plt.subplots(figsize=(5, 4))\n",
    "ax.scatter(X[:, 0], X[:, 1], s=10, alpha=0.6)\n",
    "ax.set_xlabel(r'$x_1$')\n",
    "ax.set_ylabel(r'$x_2$')\n",
    "ax.set_title('Input Data')\n",
    "plt.tight_layout()\n",
    "fig.show()\n",
    "\n",
    "# Create pytorch dataset and dataloader\n",
    "indices = torch.arange(X.shape[0])\n",
    "dataset = TensorDataset(torch.from_numpy(X).float(), indices)\n",
    "dataloader = DataLoader(dataset, batch_size=exp_params['dataset']['batch_size'], shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random samples on each of the manifolds\n",
    "np.random.seed(1)\n",
    "samples_per_manifold = exp_params['model']['K'] // 2\n",
    "print(samples_per_manifold)\n",
    "manifold_samples = []\n",
    "for i in range(2):\n",
    "    idx = np.where(y == i)[0]\n",
    "    samples = X[idx[np.random.choice(len(idx), samples_per_manifold)]]\n",
    "    manifold_samples.append(samples)\n",
    "    \n",
    "manifold_samples = torch.from_numpy(np.vstack(manifold_samples)).float()\n",
    "# Plot the samples\n",
    "fig, ax = plt.subplots(figsize=(5, 4))\n",
    "ax.scatter(X[:, 0], X[:, 1], s=10, alpha=0.4, c='orange')\n",
    "# ax.scatter(manifold_samples[:, 0], manifold_samples[:, 1], s=100, c='r', marker='x', label='Initial atoms', linewidths=2)\n",
    "ax.set_xlabel(r'$x_1$')\n",
    "ax.set_ylabel(r'$x_2$')\n",
    "ax.set_title('Input Data')\n",
    "plt.tight_layout()\n",
    "fig.savefig(results_folder / 'input_data.pdf', format='pdf')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_init = manifold_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Cluster centers for the given data\n",
    "kmeans = KMeans(n_clusters=exp_params['model']['K'], random_state=42).fit(X)\n",
    "W_init = kmeans.cluster_centers_\n",
    "# W_init /= np.linalg.norm(W_init, axis=1, keepdims=True)\n",
    "\n",
    "# sim_lam = 2*exp_params['model']['lam']\n",
    "# print(sim_lam)\n",
    "# coder = SparseCoder(dictionary=W_init, transform_alpha=exp_params['model']['lam'], transform_algorithm='lasso_lars')\n",
    "# init_codes = coder.transform(X.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KSM on manifold learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_reconstruction_error(X, D, Z):\n",
    "    \"\"\"\n",
    "    Compute the reconstruction error of the given data\n",
    "    X: (n_samples, n_features)\n",
    "    D: (n_components, n_features)\n",
    "    Z: (n_samples, n_components)\n",
    "    \"\"\"\n",
    "    prediction = Z @ D\n",
    "    error = np.mean(np.linalg.norm(X - prediction, axis=1)**2)\n",
    "    return error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models import KSM_manifold\n",
    "from src.optimizers import optimizationADMM_manifold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initializeTensors(exp_params, n_samples, Zinit=None, Pinit=None, Minit=None, seed=42, device='cpu'):\n",
    "    torch.manual_seed(seed)\n",
    "    if Zinit is None:\n",
    "        Z = torch.randn(size=(n_samples, exp_params['model']['K']), dtype=torch.float32, requires_grad=False)\n",
    "    else:\n",
    "        Z = Zinit.clone()\n",
    "        \n",
    "        \n",
    "    if Pinit is None:    \n",
    "        Z_t = Z.unsqueeze(-1)\n",
    "        V = Z_t @ Z_t.transpose(-1, -2)\n",
    "        P = V + exp_params['model']['omega'] * torch.diag_embed(Z_t.squeeze())\n",
    "    else:\n",
    "        P = Pinit.clone()\n",
    "        \n",
    "        \n",
    "    if Minit is None:        \n",
    "        M_t = torch.randn(size=P.shape, requires_grad=False, dtype=torch.float32)\n",
    "        M = M_t @ M_t.transpose(-1, -2)\n",
    "        M = M / torch.linalg.norm(M, dim=(1, 2), keepdims=True)\n",
    "    else:\n",
    "        M = Minit.clone()\n",
    "        \n",
    "        \n",
    "    # print(\"Initialized M\")\n",
    "    \n",
    "    variables = {'Z': Z, 'P': P}\n",
    "    lagrange = {'M': M}\n",
    "    \n",
    "    return variables, lagrange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_params['device'] = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device = exp_params['device']\n",
    "# M_init = torch.zeros(size=(exp_params['dataset']['n_samples'], exp_params['model']['K'], exp_params['model']['K']), dtype=torch.float32, requires_grad=False).to(device)\n",
    "# W = torch.from_numpy(W_init).float()\n",
    "W = W_init\n",
    "M_init = W @ W.T / 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sparsemax import Sparsemax\n",
    "Ps = Sparsemax(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils import power_method_svd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "M_init =  M_init.repeat(exp_params['dataset']['n_samples'], 1, 1).to(device)\n",
    "# Z_init = torch.from_numpy(init_codes).float().to(device)\n",
    "Z_init = torch.rand(size=(exp_params['dataset']['n_samples'], exp_params['model']['K']), dtype=torch.float32, requires_grad=False).to(device)\n",
    "Z_init = Ps(Z_init)\n",
    "variables, lagrange = initializeTensors(exp_params, n_samples=exp_params['dataset']['n_samples'], Zinit=Z_init, Minit=M_init, seed=42, device=device)\n",
    "exp_params['optimizer']['max_epochs'] = 5000\n",
    "exp_params['optimizer']['latent_iters'] = 15\n",
    "exp_params['optimizer']['log_interval'] = 20\n",
    "exp_params['optimizer']['lrs']['params'] = 2e-3\n",
    "exp_params['optimizer']['param_iters'] = 1\n",
    "exp_params['model']['omega'] = 0.1\n",
    "model = KSM_manifold(exp_params, W_init=W)\n",
    "eta = 1/ power_method_svd(model.W.T, device=device)\n",
    "optimizer = optimizationADMM_manifold(exp_params, model, eta=eta)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dataset': {'batch_size': 128, 'dim': 30, 'n_samples': 2500, 'name': 'moons'},\n",
      " 'device': device(type='cuda'),\n",
      " 'model': {'K': 12,\n",
      "           'Winit': 'Kmeans',\n",
      "           'omega': 0.1,\n",
      "           'perturbation': 0.1,\n",
      "           'rho': 1.0,\n",
      "           'sparsity': 0.125},\n",
      " 'optimizer': {'Minit': 'zero',\n",
      "               'latent_iters': 15,\n",
      "               'log_interval': 20,\n",
      "               'lrs': {'Z': 0.01,\n",
      "                       'Z_decay': 0.9,\n",
      "                       'interval': 20,\n",
      "                       'params': 0.002},\n",
      "               'max_epochs': 5000,\n",
      "               'param_iters': 1,\n",
      "               'vis_interval': 20}}\n"
     ]
    }
   ],
   "source": [
    "current_path = Path.cwd()\n",
    "results_folder = current_path / 'results'/f'{datetime.now().strftime(\"%m-%d\")}/{datetime.now().strftime(\"%H-%M\")}'\n",
    "os.makedirs(results_folder, exist_ok=True)\n",
    "from copy import deepcopy\n",
    "cfg = deepcopy(exp_params)\n",
    "\n",
    "# folder = project_dir / f'results/simulated/highdim/{datetime.now().strftime(\"%m-%d\")}/{datetime.now().strftime(\"%H-%M\")}'\n",
    "os.makedirs(results_folder, exist_ok=True)\n",
    "cfg.pop('device')\n",
    "cfg = OmegaConf.create(cfg)\n",
    "with open(results_folder / 'config.yaml', 'w') as f:\n",
    "    OmegaConf.save(cfg, f)\n",
    "    \n",
    "pprint(exp_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.admms import run_ADMM_kds_Ptrack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/5000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 5000/5000 [29:39<00:00,  2.81it/s, Total Loss: -0.7329 | Similarity Loss: -0.7334 | Penalty: 0.0000 | R2_W: 0.9978 | R2_est: 0.9861 | D_sim: 0.0000 | Z_sim: 0.0000 | DW_sim: 0.0000 | new_eta: 0.0839]\n"
     ]
    }
   ],
   "source": [
    "# true_vals = {'D': D, 'Z': Z}\n",
    "\n",
    "loss_vals = run_ADMM_kds_Ptrack(model, optimizer, torch.from_numpy(X).float(), variables, lagrange, dataloader, exp_params,\n",
    "                                device=device, update_eta=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's plot the losses\n",
    "fig, axs = plt.subplots(1, 4, figsize=(20, 5))\n",
    "iterations = np.arange(0, exp_params['optimizer']['max_epochs'], step=exp_params['optimizer']['log_interval'])\n",
    "ax = axs[0]\n",
    "ax.plot(iterations[1:], loss_vals['total_loss'][1:])\n",
    "ax.set_title('Total Loss')\n",
    "# ax.set_xscale('log')\n",
    "ax.set_xlabel('Iterations')\n",
    "ax.set_ylabel('Loss')\n",
    "\n",
    "ax = axs[1]\n",
    "ax.plot(iterations[1:], loss_vals['similarity_loss'][1:])\n",
    "ax.set_title('Similarity Loss')\n",
    "# ax.set_xscale('log')\n",
    "ax.set_xlabel('Iterations')\n",
    "\n",
    "# ax = axs[2]\n",
    "# ax.plot(iterations, loss_vals['sparsity'])\n",
    "# ax.set_title('Sparsity Measure')\n",
    "# # ax.set_xscale('log')\n",
    "# ax.set_xlabel('Iterations')\n",
    "\n",
    "ax = axs[2]\n",
    "ax.plot(iterations, loss_vals['penalty'])\n",
    "ax.set_title('Penalty')\n",
    "# ax.set_xscale('log')\n",
    "ax.set_xlabel('Iterations')\n",
    "\n",
    "\n",
    "\n",
    "# ax.plot(loss_vals['loss_Z'])\n",
    "\n",
    "\n",
    "# Plot the r2 score and similarity measures\n",
    "# fig, axs = plt.subplots(1, 2, figsize=(15, 5))\n",
    "ax = axs[3]\n",
    "# ax.plot(iterations[1:], loss_vals['r2_W'][1:], label=r'$R^2 (W)$')\n",
    "ax.plot(iterations[1:], loss_vals['r2_est'][1:], label=r'$R^2$ (Estimated)')\n",
    "# ax.set_xscale('log')\n",
    "ax.set_title('R2 Score')\n",
    "ax.set_xlabel('Iterations')\n",
    "ax.set_ylabel('R2 Score')\n",
    "ax.legend()\n",
    "\n",
    "# ax = axs[1]\n",
    "# ax.plot(iterations, loss_vals['dw_sim'], label='cos sim (W)')\n",
    "# ax.plot(iterations, loss_vals['dest_sim'], label='cos sim (est)')\n",
    "# # ax.set_xscale('log')\n",
    "# ax.set_title('Cos Similarity for estimated dictionary')\n",
    "# ax.set_xlabel('Iterations')\n",
    "# ax.set_ylabel('Value')\n",
    "# ax.legend()\n",
    "\n",
    "# ax = axs[2]\n",
    "# ax.plot(iterations, loss_vals['latent_sim'])\n",
    "# # ax.set_xscale('log')\n",
    "# ax.set_title('Cos Similarity for latent codes')\n",
    "# ax.set_xlabel('Iterations')\n",
    "# ax.set_ylabel('Value')\n",
    "\n",
    "# fig.savefig(folder / 'r2_n_cossim.pdf', format='pdf', bbox_inches='tight')\n",
    "fig.savefig(results_folder / 'loss_curves.pdf', format='pdf', bbox_inches='tight')\n",
    "\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the data and reconstruction\n",
    "fig, ax = plt.subplots(figsize=(6, 4))\n",
    "D_W = model.W.detach().cpu().numpy() \n",
    "H_est = torch.mean(variables['P'], dim=0).detach().cpu()\n",
    "Z_est = variables['Z'].detach().cpu()\n",
    "D_est = (1 + model.omega) * torch.linalg.solve(H_est, Z_est.T) @ torch.from_numpy(X).float() / X.shape[0]\n",
    "XhatW = Z_est @ D_W\n",
    "Xhat = Z_est @ D_est\n",
    "ax.scatter(X[:, 0], X[:, 1], s=10, alpha=0.4, label='Orginal Data', c='orange')\n",
    "# ax.scatter(XhatW[:, 0], XhatW[:, 1], s=10, alpha=0.6, c='r', label='Reconstruction (W)')\n",
    "ax.scatter(XhatW[:, 0], XhatW[:, 1], s=10, alpha=0.3, c='g', label='Reconstruction')\n",
    "ax.scatter(D_est[:, 0], D_est[:, 1], s=100, c='r', marker='x', label='Estimated Dictionary', linewidths=3)\n",
    "# ax.scatter(W_init[:, 0], W_init[:, 1], s=100, c='b', marker='X', label='Init Dictionary', linewidths=1)\n",
    "# ax.scatter(D_est[:, 0], D_est[:, 1], s=100, c='y', marker='X', label='Estimated Dictionary (P)', linewidths=1)\n",
    "ax.set_title('Manifold Reconstruction')\n",
    "ax.legend()\n",
    "fig.savefig(results_folder / 'moons.pdf', format='pdf', bbox_inches='tight')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram of the support of the latent codes\n",
    "fig, ax = plt.subplots(figsize=(6, 4))\n",
    "# Compute the support of the latent codes\n",
    "support = torch.sum((variables['Z'] > 1e-3).float(), dim=1)\n",
    "# plot the histogram with border\n",
    "\n",
    "sns.histplot(support.cpu().numpy(), bins=20, ax=ax, edgecolor='black', linewidth=1.2)\n",
    "ax.set_title('Histogram of significant activations capture intrinsic dim.')\n",
    "ax.set_xlabel('Support')\n",
    "ax.set_ylabel('Frequency')\n",
    "fig.savefig(results_folder / 'support_hist.pdf', format='pdf', bbox_inches='tight')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(results_folder / 'results.pkl', 'wb') as f:\n",
    "    pickle.dump({'loss_vals': loss_vals, 'model': model, 'variables': variables, 'lagrange': lagrange}, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bioplausible-learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
